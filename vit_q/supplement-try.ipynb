{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Analysis\n",
    "> BF15 Numerical Accuracy\n",
    "\n",
    "> Date.11/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T00:09:12.811235Z",
     "iopub.status.busy": "2025-11-05T00:09:12.810671Z",
     "iopub.status.idle": "2025-11-05T00:09:12.823386Z",
     "shell.execute_reply": "2025-11-05T00:09:12.822368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n",
      "Python path: /usr/local/bin/python\n",
      "True\n",
      "NVIDIA A100-PCIE-40GB\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "# Env info\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Python path:\", sys.executable)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T00:09:12.891692Z",
     "iopub.status.busy": "2025-11-05T00:09:12.891260Z",
     "iopub.status.idle": "2025-11-05T00:09:16.141994Z",
     "shell.execute_reply": "2025-11-05T00:09:16.141017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torchvision import transforms\n",
    "from bf15_linear import replace_linear_with_bf15, bf15_left_matmul\n",
    "from bfspmat import replace_linear_with_bf15 as replace_linear_with_bf15_full\n",
    "from bfspmat import bf15_left_exp_int_matmul\n",
    "from vit import ImageNetValDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Error Budget Analysis\n",
    "- Max ULP (Units in the Last Place): The largest difference between the simulated BF15 and FP32 results.\n",
    "- Mean ULP: The average difference across all layers.\n",
    "- Histogram of ULPs: To visualize the distribution of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T00:09:16.819571Z",
     "iopub.status.busy": "2025-11-05T00:09:16.819289Z",
     "iopub.status.idle": "2025-11-05T00:09:17.317448Z",
     "shell.execute_reply": "2025-11-05T00:09:17.316710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max ULP (LSB Only): 0.3598213195800781\n",
      "Mean ULP (LSB Only): 0.05452027916908264\n",
      "Max ULP (LSB + Post-Alignment): 0.2872352600097656\n",
      "Mean ULP (LSB + Post-Alignment): 0.051935825496912\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1. Error Budget Analysis\n",
    "# ==========================================\n",
    "def compute_error_budget(A, B, tile_k=64):\n",
    "    \"\"\"\n",
    "    Compute the error budget for BF15 truncation and post-alignment.\n",
    "    \"\"\"\n",
    "    # FP32 reference\n",
    "    y_ref = torch.matmul(A.to(torch.float32), B.to(torch.float32))\n",
    "\n",
    "    # BF15 with LSB truncation only\n",
    "    y_bf15_lsb = bf15_left_matmul(A, B)\n",
    "\n",
    "    # BF15 with LSB truncation + post-alignment\n",
    "    y_bf15_full = bf15_left_exp_int_matmul(A, B, tile_k=tile_k)\n",
    "\n",
    "    # Compute ULP differences\n",
    "    ulp_lsb = (y_bf15_lsb - y_ref).abs()\n",
    "    ulp_full = (y_bf15_full - y_ref).abs()\n",
    "\n",
    "    return ulp_lsb, ulp_full, y_ref, y_bf15_lsb, y_bf15_full\n",
    "\n",
    "# Generate random input tensors\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "A = torch.randn(128, 256, dtype=torch.bfloat16, device=device)\n",
    "B = torch.randn(256, 128, dtype=torch.bfloat16, device=device)\n",
    "\n",
    "# Compute error budget\n",
    "ulp_lsb, ulp_full, y_ref, y_bf15_lsb, y_bf15_full = compute_error_budget(A, B)\n",
    "\n",
    "# Plot histogram of ULPs\n",
    "density = False # Normalize the histogram\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(ulp_lsb.cpu().numpy().flatten(), bins=50, alpha=0.5, label=\"LSB Truncation Only\", density=density)\n",
    "plt.hist(ulp_full.cpu().numpy().flatten(), bins=50, alpha=0.5, label=\"LSB + Post-Alignment\", density=density)\n",
    "plt.xlabel(\"ULP Error\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Error Budget: ULP Distribution)\")\n",
    "plt.savefig(\"error_budget_ulp.svg\", format=\"svg\")\n",
    "plt.show()\n",
    "\n",
    "# Print max and mean ULPs\n",
    "print(f\"Max ULP (LSB Only): {ulp_lsb.max().item()}\")\n",
    "print(f\"Mean ULP (LSB Only): {ulp_lsb.mean().item()}\")\n",
    "print(f\"Max ULP (LSB + Post-Alignment): {ulp_full.max().item()}\")\n",
    "print(f\"Mean ULP (LSB + Post-Alignment): {ulp_full.mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Ablation Study\n",
    "- Compare the outputs of the following configurations:\\\n",
    "FP32 baseline: Full precision.\\\n",
    "BF15 with LSB truncation only: Using `bf15_linear.py`.\\\n",
    "BF15 with LSB truncation + post-alignment: Using `bfspmat.py`.\n",
    "- Measure the accuracy (Top-1, Top-5) and inference time for each configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz`\n",
    "\n",
    "`tar -xvzf imagenette2-320.tgz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T00:09:17.321508Z",
     "iopub.status.busy": "2025-11-05T00:09:17.321038Z",
     "iopub.status.idle": "2025-11-05T00:09:17.325889Z",
     "shell.execute_reply": "2025-11-05T00:09:17.325223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_fp32 = evaluate_model(model_fp32, dataloader, device)\n",
      "\n",
      "FP32 Accuracy: Top-1 16.00%, Top-5 16.00%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "acc_bf15_lsb = evaluate_model(model_bf15_lsb, dataloader, device)\n",
      "\n",
      "BF15 (LSB Only) Accuracy: Top-1 16.00%, Top-5 16.00%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "acc_bf15_full = evaluate_model(model_bf15_full, dataloader, device)\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.93 GiB. GPU 0 has a total capacity of 39.49 GiB of which 4.66 GiB is free. Process 1896216 has 34.82 GiB memory in use. Of the allocated memory 34.05 GiB is allocated by PyTorch, and 286.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33macc_bf15_full = evaluate_model(model_bf15_full, dataloader, device)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m acc_bf15_full = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_bf15_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBF15 (LSB + Post-Alignment) Accuracy: Top-1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_bf15_full[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%, Top-5 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_bf15_full[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, dataloader, device, micro_batch_size)\u001b[39m\n\u001b[32m     75\u001b[39m imgs_micro = imgs[i:i+micro_batch_size]\n\u001b[32m     76\u001b[39m targets_micro = targets[i:i+micro_batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_micro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m total += targets_micro.size(\u001b[32m0\u001b[39m)\n\u001b[32m     79\u001b[39m _, pred = outputs.topk(\u001b[32m5\u001b[39m, \u001b[32m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py:298\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    295\u001b[39m batch_class_token = \u001b[38;5;28mself\u001b[39m.class_token.expand(n, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    296\u001b[39m x = torch.cat([batch_class_token, x], dim=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[32m    301\u001b[39m x = x[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py:157\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    155\u001b[39m torch._assert(\u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m3\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m + \u001b[38;5;28mself\u001b[39m.pos_embedding\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ln(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:245\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py:118\u001b[39m, in \u001b[36mEncoderBlock.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    115\u001b[39m x = x + \u001b[38;5;28minput\u001b[39m\n\u001b[32m    117\u001b[39m y = \u001b[38;5;28mself\u001b[39m.ln_2(x)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x + y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:245\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Essay-Hy-FPCIM/vit_q/bfspmat.py:159\u001b[39m, in \u001b[36mBF15IntLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    157\u001b[39m x2d = x.reshape(-\u001b[32m1\u001b[39m, x.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m#  BF15  matmul\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m y2d = \u001b[43mbf15_left_exp_int_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    161\u001b[39m     y2d = (y2d.to(torch.float32) + \u001b[38;5;28mself\u001b[39m.bias.to(torch.float32)).to(torch.bfloat16)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Essay-Hy-FPCIM/vit_q/bfspmat.py:74\u001b[39m, in \u001b[36mbf15_left_exp_int_matmul\u001b[39m\u001b[34m(A, B, tile_k)\u001b[39m\n\u001b[32m     70\u001b[39m Be_t = Be.reshape(T, tile_k, N).contiguous()                       \u001b[38;5;66;03m# [T, tile_k, N]\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# 3)  tile  GPU \u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# prod: [T, M, tile_k, N]   int32\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m prod  = \u001b[43mAm_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mBm_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m                     \u001b[38;5;66;03m# (T,M,t,N)\u001b[39;00m\n\u001b[32m     75\u001b[39m e_sum = Ae_t.unsqueeze(-\u001b[32m1\u001b[39m) + Be_t.unsqueeze(\u001b[32m1\u001b[39m)                     \u001b[38;5;66;03m# (T,M,t,N)\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# 4) E_ref_tile = max_k(e_sum)\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 6.93 GiB. GPU 0 has a total capacity of 39.49 GiB of which 4.66 GiB is free. Process 1896216 has 34.82 GiB memory in use. Of the allocated memory 34.05 GiB is allocated by PyTorch, and 286.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. Ablation Study\n",
    "# ==========================================\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from bf15_linear import replace_linear_with_bf15\n",
    "from bfspmat import replace_linear_with_bf15 as replace_linear_with_bf15_full\n",
    "from vit import ImageNetValDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Load ViT model\n",
    "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_fp32 = vit_b_16(weights=weights).eval().to(device)\n",
    "\n",
    "# Clone models for BF15 configurations\n",
    "model_bf15_lsb = vit_b_16(weights=weights).eval()\n",
    "model_bf15_full = vit_b_16(weights=weights).eval()\n",
    "\n",
    "# Replace Linear layers\n",
    "replace_linear_with_bf15(model_bf15_lsb)  # LSB truncation only\n",
    "model_bf15_lsb.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "replace_linear_with_bf15_full(model_bf15_full)  # LSB + post-alignment\n",
    "model_bf15_full.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Load ImageNet validation dataset\n",
    "imagenet_root = \"./imagenette2-320/val\"  # Path to the validation dataset\n",
    "\n",
    "# Generate wnid_map directly from the folder structure\n",
    "def generate_wnid_map(root_dir):\n",
    "    classes = sorted(os.listdir(root_dir))\n",
    "    wnid_map = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    return wnid_map\n",
    "\n",
    "wnid_map = generate_wnid_map(imagenet_root)\n",
    "\n",
    "# Preprocessing and dataset\n",
    "# Define preprocess\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "subset_size = 50  # Use only 50 samples for evaluation\n",
    "dataset = ImageNetValDataset(imagenet_root, wnid_map, preprocess)\n",
    "subset_indices = torch.randperm(len(dataset))[:subset_size]\n",
    "subset = torch.utils.data.Subset(dataset, subset_indices)\n",
    "dataloader = DataLoader(subset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# Evaluate models\n",
    "# def evaluate_model(model, dataloader, device):\n",
    "#     total, correct_top1, correct_top5 = 0, 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for imgs, targets in dataloader:\n",
    "#             imgs, targets = imgs.to(device), targets.to(device)\n",
    "#             outputs = model(imgs)\n",
    "#             total += targets.size(0)\n",
    "#             _, pred = outputs.topk(5, 1, True, True)\n",
    "#             correct = pred.eq(targets.view(-1, 1).expand_as(pred))\n",
    "#             correct_top1 += correct[:, :1].sum().item()\n",
    "#             correct_top5 += correct[:, :5].sum().item()\n",
    "#     acc1 = correct_top1 / total * 100\n",
    "#     acc5 = correct_top5 / total * 100\n",
    "#     return acc1, acc5\n",
    "def evaluate_model(model, dataloader, device, micro_batch_size=4):\n",
    "    total, correct_top1, correct_top5 = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in dataloader:\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "            batch_size = imgs.size(0)\n",
    "            for i in range(0, batch_size, micro_batch_size):\n",
    "                imgs_micro = imgs[i:i+micro_batch_size]\n",
    "                targets_micro = targets[i:i+micro_batch_size]\n",
    "                outputs = model(imgs_micro)\n",
    "                total += targets_micro.size(0)\n",
    "                _, pred = outputs.topk(5, 1, True, True)\n",
    "                correct = pred.eq(targets_micro.view(-1, 1).expand_as(pred))\n",
    "                correct_top1 += correct[:, :1].sum().item()\n",
    "                correct_top5 += correct[:, :5].sum().item()\n",
    "    acc1 = correct_top1 / total * 100\n",
    "    acc5 = correct_top5 / total * 100\n",
    "    return acc1, acc5\n",
    "\n",
    "\n",
    "# Evaluate all configurations\n",
    "print(\"acc_fp32 = evaluate_model(model_fp32, dataloader, device)\\n\")\n",
    "acc_fp32 = evaluate_model(model_fp32, dataloader, device)\n",
    "print(f\"FP32 Accuracy: Top-1 {acc_fp32[0]:.2f}%, Top-5 {acc_fp32[1]:.2f}%\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "\n",
    "print(\"acc_bf15_lsb = evaluate_model(model_bf15_lsb, dataloader, device)\\n\")\n",
    "acc_bf15_lsb = evaluate_model(model_bf15_lsb, dataloader, device)\n",
    "print(f\"BF15 (LSB Only) Accuracy: Top-1 {acc_bf15_lsb[0]:.2f}%, Top-5 {acc_bf15_lsb[1]:.2f}%\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "\n",
    "print(\"acc_bf15_full = evaluate_model(model_bf15_full, dataloader, device)\\n\")\n",
    "acc_bf15_full = evaluate_model(model_bf15_full, dataloader, device)\n",
    "print(f\"BF15 (LSB + Post-Alignment) Accuracy: Top-1 {acc_bf15_full[0]:.2f}%, Top-5 {acc_bf15_full[1]:.2f}%\")\n",
    "print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
